/** @page ulrPage Unstructured Lumigraph Rendering (ULR)
  *
  * @ingroup groupeIBR
  * \tableofcontents
  *
  * \section secDataStruct Data structure and render passes
  *
  * \subsection subsecSpixelInputDataSetFormats SpixelWarp input dataset formats
  */
  * You can use the src/submodule/ibr-dataset-generation project to generate the dataset !
  *
  * |  			files									| 		comeFrom (if not from dataset generation process)		 						  | format (Text/Binary) |
  * | :-----------------------------------------------: | :---------------------------------------------: | :------------------: |
  * | \ref subsecSpixelInputDataSetFormatsImg			| computed from [RadialUndistort](http://www.cs.cornell.edu/~snavely/bundler/bundler-v0.4-manual.html#S3) or equivalent [VSFM] (http://ccwu.me/vsfm/)... | Jpg |
  * | \ref subsecSpixelInputDataSetFormatsBundle		| computed from [bundler](http://www.cs.cornell.edu/~snavely/bundler/) or equivalent [VSFM] (http://ccwu.me/vsfm/)... | Txt |
  * | \ref subsecSpixelInputDataSetFormatsListImg		| computed from hand | Txt |
  * | \ref subsecSpixelInputDataSetFormatsClipPlanes	| computed from the depth maps (see \ref pageReprojection) | Txt |
  * | \ref subsecSpixelInputDataSetFormatsActiveImg		| create by hand to list/select/filter all input images we want to process| Txt (optional)|
  * | \ref subsecSpixelInputDataSetFormatsSp			| computed from superpixel project (SLIC algo) (see \ref pageSuperPixel) | Bin |
  * | \ref subsecSpixelInputDataSetFormatsSpGraph		| computed from superpixel project (see \ref pageSuperPixel) | Txt |
  * | \ref subsecSpixelInputDataSetFormatsDepthMVS		| computed from matlab stuff propagation depth synthesis (see \ref pagePropagation)| Txt(ascii) |
  * | \ref subsecSpixelInputDataSetFormatsDepthDepth	| computed from matlab stuff propagation depth synthesis (see \ref pagePropagation)| Png|
  *
  *
  * \subsection subsecSpixelInputDataSetFormatsBundle bundle.out
  *  Content: [the bundler documentation](http://www.cs.cornell.edu/~snavely/bundler/bundler-v0.4-manual.html) explain all what it contain \n
  *	 Description: [the output of the bundler](http://www.cs.cornell.edu/~snavely/bundler/bundler-v0.4-manual.html#S6) \n
  * Bundle file format is in plain text :\n
  * ~~~~~~~~~~~~~{.txt}
  * # Bundle file v0.3
  * <num_cameras> <num_points>   [two integers]
  * <camera1>
  * <camera2>
  *   ...
  * <cameraN>
  * <point1>
  * <point2>
  *    ...
  * <pointM>
  * ~~~~~~~~~~~~~
  * Where <camera> contain :
  * ~~~~~~~~~~~~~{.txt}
  * <f> <k1> <k2>   [the focal length, followed by two radial distortion coeffs]
  * <R>             [a 3x3 matrix representing the camera rotation]
  * <t>             [a 3-vector describing the camera translation]
  * ~~~~~~~~~~~~~
  * And where <point> contain :
  * ~~~~~~~~~~~~~{.txt}
  * <position>      [a 3-vector describing the 3D position of the point]
  * <color>         [a 3-vector describing the RGB color of the point]
  * <view list>     [a list of views the point is visible in]
  * ~~~~~~~~~~~~~
  *
  *
  * \subsection subsecSpixelInputDataSetFormatsListImg list_images.txt
  *  Content: It a list of all input images sorted by order it was taken (renamed) with their resolution \n
  *  Description: If you have [ImageMagick](http://www.imagemagick.org/script/command-line-options.php#format_identify_) you can do : `identify -format "%f %w %h\n" *.jpg` \n
  * ~~~~~~~~~~~~~{.txt}
  * <%8d.jpg> <width> <height>
  * ~~~~~~~~~~~~~
  * Example:
  * ~~~~~~~~~~~~~{.txt}
  * 00000000.jpg 2256 1504
  * 00000001.jpg 2256 1504
  * 00000002.jpg 2256 1504
  * ...
  * 00000026.jpg 2256 1504
  * 00000027.jpg 2256 1504
  * ~~~~~~~~~~~~~
  *
  *
  * \subsection subsecSpixelInputDataSetFormatsClipPlanes clipping_planes.txt
  *  Content: The near and the far clipping planes informations compute from the nearest and the farest 3D reconstructed points\n
  *  Description: We cumulate all the depth maps files and extract the near and far clipping plane for the virtual camera.\n
  * ~~~~~~~~~~~~~{.txt}
  * <near> <far>	[two floats]
  * ~~~~~~~~~~~~~
  *
  *
  * \subsection subsecSpixelInputDataSetFormatsActiveImg active_images.txt
  *  Content: If we want to select which image we want to process, we create this file and put all input images indexes
  *  Description:
  * ~~~~~~~~~~~~~{.txt}
  * <ActiveImageID> <ActiveImageID> <ActiveImageID> <...>
  * ~~~~~~~~~~~~~
  *
  *
  * \subsection subsecSpixelInputDataSetFormatsSp superpixel/%8d.sp (superpixel/00000000.sp)
  *  Content: This is the oversegmented image where each pixel of a superpixel (segmented from SLIC superpixel project (\ref pageSuperPixel)) have a unique ID\n
  *  Description: <b>Be careful, this is a binary file, so the description here is just to have a representation of its content.</b>\n
  * ~~~~~~~~~~~~~{.txt}
  * <width> <height> [resolution as tow intergers]
  * <pixel(0,height)=spixelID> <pixel(1,height)=spixelID> <pixel(2,height)=spixelID> <...> <pixel(widht,height)=spixelID> [ID as an interger]
  *  ...
  * <pixel(0,0)=spixelID> <pixel(1,0)=spixelID> <pixel(2,0)=spixelID> <...> <pixel(widht,0)=spixelID> [ID as an interger]
  * ~~~~~~~~~~~~~
  *
  *
  * \subsection subsecSpixelInputDataSetFormatsSpGraph superpixel/spixel_graph.txt
  *  Content: A graph of neighbors for each superpixel overall segmented images \n
  *  Description: Since it's an ascii file, line breaks are clearly marked. \n
  * ~~~~~~~~~~~~~{.txt}
  * <totalSuperpixelsNumber> \n [over all images]
  * <numberOfNeighbors> <<idCamNeighbor1> <idSuperpixelNeighbor1>> ... <<idCamNeighborN> <idSuperpixelNeighborN>> \n [for first superpixel]
  * <numberOfNeighbors> <<idCamNeighbor1> <idSuperpixelNeighbor1>> ... <<idCamNeighborN> <idSuperpixelNeighborN>> \n [for second superpixel]
  * ...
  * <numberOfNeighbors> <<idCamNeighbor1> <idSuperpixelNeighbor1>> ... <<idCamNeighborN> <idSuperpixelNeighborN>> \n [for last superpixel]
  * ~~~~~~~~~~~~~
  *
  *
  * \subsection subsecSpixelInputDataSetFormatsDepthMVS depth/%8d.mvs (depth/00000000.mvs)
  *  Content: This file contain all informations about back projected reconstructed points cloud and additional points depth synthesis \n
  *  Description: Since it's an ascii file, line breaks are clearly marked. \n
  * ~~~~~~~~~~~~~{.txt}
  * <widht> <height> <numOfSuperpixels>
  * <superpixel1> \n
  * <superpixel2> \n
  * ...
  * <superpixelN> \n
  * ~~~~~~~~~~~~~
  * Where <superpixelI> contains (space delimited) :
  * ~~~~~~~~~~~~~{.txt}
  * <idSuperpixel> <numberOfPoints> <medianDepth> <point1> <point2> ... <pointN>
  * ~~~~~~~~~~~~~
  * Where <pointI> contain :
  * ~~~~~~~~~~~~~{.txt}
  * <x> <y> <depth> [in range -1;1]
  * ~~~~~~~~~~~~~
  *
  *
  * \subsection subsecSpixelInputDataSetFormatsDepthDepth depth/depth_%8d.png (depth/depth_00000000.png)
  *  Content: This file is only used for the render top view (see Reproject#Reproject). It load it to re-projected 3D points cloud according to depth (gray scale) information in the image\n
  *  Description: \n
  *
  *
  *
  *
  *
  * \subsection subsecSpixelDataStruct Superpixels data structure
  *
  * \li SPixelImage class load the result of the oversegmentation (one instance by input image)
  * \li Each pixel within the oversegmented image with the same value \b id (SPixelImage#_id) is a part of a new SPixel with a \b global_id SPixel#_gid (over all images)
  * \li SPixelImage#map compute a data texture which contain for each pixel of the oversegmented image the current global id of the pixel and its 3 closest neighbors : \n
  *    in canal R: the current SPixel#_gid associated to the pixel \n
  *    in canal G: the neighbor SPixel#_gid if depth diff > 0.08 (otherwise repeat the current SPixel#_gid of the supperpixel) \n
  *    in canal B: the neighbor SPixel#_gid if depth diff > 0.08 (otherwise repeat the current SPixel#_gid of the supperpixel) \n
  *    in canal A: the neighbor SPixel#_gid if depth diff > 0.08 (otherwise repeat the current SPixel#_gid of the supperpixel) \n
  * \image html superpixel_cmap_data_structure.png "Fig.1 : depth superpixels proximities data image"
  * <b>From the Fig.1 :</b>We have a texture where each pixel have 4 components which correspond to the best closest depth proximities for a pixel coord.\n
  * This will be useful at render time for \ref subsecPass1 in warp.fp.
  *
  * \section secRenderStep Render step (for each novel view)
  *
  * \subsection subsecPass1 first render pass (camera selection and shape-preserving warp):
  * \li We select the 4 closest input cameras of the new point of view (novel camera we want to render) => See RendererGL#select_warp_images \n
  *
  * <b>Then for each closest cameras (RendererGL#render_warped):</b>
  *
  * \li We solve sparse linear system for each local warps (each superpixel transformations vertices computation) => ImageWarper#warp \n
  *    \image html warpMesh.png "Fig.2 : mesh for shape preserving warp"
  *    In fact, from the initialisation (before render loop), we overlay a 2D boundingBox on each superpixel of each images => see ImageWarper#init. \n
  *    Little trick: we take a little wider (about 1%) Bbox to avoid depth sample lying on boundary.\n
  *    We took some random points inside our superpixels. \n
  *    This will give us vertices we used (in addition of 4 corner BBox) to triangulate a 2D mesh over our superpixels. \n
  *    Note that vertices are not necessary a point with depth information. Triangles may contain samples depth or not.\n
  *    Then, each superpixel of an image have a mesh associate to the input view matrix. \n
  *    With the novel view matrix we can find the vertices transformations to preserve shape in the novel view. \n
  *    Then we can assign a unique index number for each vertices of each mesh of superpixels to get one big single Vertex Buffer Object to process. \n
  *    We filled the vertex array (coord between [-1;1]). \n
  *    We filled the coord text array [u;v] between [0;1] \b AND median depth of mesh corresponding superpixel \b AND the superpixel global_id \n
  *    Texture coord had 4 components [u;v;d;gid].
  *
  * \li We render the warped image of the novel view (ImageWarper#renderWarp) using shader (ibr.vp and warp.fp)\n
  * With prepared data structures, on glDrawElements call, our vertex shader and fragment shader can works : \n
  * \b ibr.vp : take vertex from vertices and coordTexture of this vertex (which contain extra infos [u;v;d;gid]).\n
  * It disable the ModelViewMatrix projection, put the tranformed vertex and pass the coordtext to the warp.fp.\n
  * Note that the vec4 color will be used for another purpose (see RendererGL#render_top_view).\n
  * \b warp.fp : take the input image and the oversegmented image (both resized to the window size) and the vec4 coordtext fro the ibr.vp.\n
  * Here we test if the pixel we have to render is a part of the current superpixel. \n
  * Indeed, we try to render pixel from the superpixel mesh based on a Bbox, but all pixels of the Bbox are not necessarly a part of the superpixel.\n
  * So we need to check the global_id of the pixel and for those pixel outside the superpixel, we need to test if the depth of one of the 3 best neighbors superpixel match the depth of the current superpixel. \n
  * Depth evaluations has already been done at the begining and can be retrieved from the \ref subsecSpixelDataStruct. \n
  * So, with textCoord we can access a pixel of the texture data and recover the 3 closest superpixel global_id. \n
  * <b>The objective here is to fill cracks between warped superpixel due to the transformation.</b> \n
  * So, only for pixels on "out-border" of the superpixel and in a "same depth" neighbors superpixel, we put the depth information and take the RGB value of the input image to expose as output.\n
  * Note that we also expose the Warped texture coords, the current superpixel id and the depth info in a vec4.\n
  * Otherwise, we discard the pixel render.\n
  * \image html warp_pixel_shader.png "Fig.3 : warp pixel shader"
  * <b>From the Fig.3 :</b> We evaluate the mesh of the red target superpixel to be rendered.\n
  * We need to see if pixel (inside the mesh but outside the superpixel area) have to be drawn or not.\n
  * We take the depth superpixels proximities data image from Fig.1 and extract eligible global_id neighbors for that pixel in order to compare with the current global_id red target superpixel we are processing.\n
  * This is possible thanks to our current coordText we took from our previous ibr.vp.\n
  * We found correspondance ? yes => we continue the draw process for that pixel; no otherwise => we cancel the process for that pixel.\n
  *
  * \subsection subsecPass2 second render pass (blending from warped images):
  * Here the main objective is, for each pixel of superpixel warped (from 4 previous render target images), we want to select only the 2 best superpixel candidate to blend together with a weight.\n
  * Selection and weight criteras are :\n
  * 1- Computing angle penality to keep only 2 best candidates [Buehler et al. 2001]\n
  * 2- Check the superpixel graph correspondance (across all images)\n
  *   2.1- If the pixels to be blended have a correspondence edge (neighbor), we use the weights computed above to blend both superpixel\n
  *   2.2- If superpixels do not have correspondence and 1 superpixel contains "true" depth samples (obtained from PMVS/VSFM) while the other contains depth samples added by our synthesis, we increase the weight of the former by a factor of 2.0\n
  *   2.3- In all other cases, we increase the blending weight of the pixel with the higher depth value by a factor of 2.0\n
  *
  * \subsection subsecPass3 third render pass (hole filling):
  * Here, with a "ping-pong" shader we implement the POISSON hole filling.\n
  * We solve the Poisson equation [Perez et al. 2003] with zero gradient values to create blurred color in such holes.
  *
  */
